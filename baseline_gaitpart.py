# -*- coding: utf-8 -*-
"""baseline-gaitpart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lGH4CL_I_aDA4Lga01wYtrX6sKlltZrx

# Import Packages
"""

import numpy as np
import os
from google.colab import drive
drive.mount('/content/drive')
os.chdir('/content/drive/MyDrive')

"""# Load Data"""

# Load the data from the .npy file:
data = np.load("CASIA-verification.npy", allow_pickle=True)

# Split the data back into separate arrays:
X_train_array, X_test_array, y_train_array, y_test_array = data

y_test_array[10]

# distance_train = np.load("train2.npy", allow_pickle=True)
# distance_test = np.load("test2.npy", allow_pickle=True)

X_train_array.shape

X_train, X_test, y_train, y_test = X_train_array[0], X_test_array[0], y_train_array[0], y_test_array[0]
y_train = np.array(y_train[:],dtype='uint8')
y_train = np.eye(2)[y_train].reshape((y_train.shape[0],-1))

y_test = np.array(y_test[:],dtype='uint8')
y_test = np.eye(2)[y_test].reshape((y_test.shape[0],-1))

# Transpose the array to swap the 2nd and 4th dimensions
print(X_train.shape, X_test.shape)
X_train = np.transpose(X_train, axes=[0, 3, 1, 2])  # shape: (48, 100, 36, 20)
X_test  = np.transpose(X_test , axes=[0, 3, 1, 2])  # shape: (12, 100, 36, 20)

# Reshape the array to combine the 2nd and 3rd dimensions
# X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], -1))  # shape: (48, 100, 720)
# X_test  = np.reshape(X_test , (X_test.shape[0], X_test.shape[1], -1))  # shape: (12, 100, 720)

# X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1], X_train.shape[2], -1))  # shape: (48, 100, 720)
# X_test  = np.reshape(X_test , (X_test.shape[0], 1, X_test.shape[1], X_test.shape[2], -1))  # shape: (12, 100, 720)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

import torch
from torch.utils.data import Dataset

class CasiaDataset(Dataset):
    def __init__(self, x_data, y_data):
        self.x_data = x_data
        self.y_data = y_data

    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        x_sample = self.x_data[idx]
        y_sample = self.y_data[idx]
        return x_sample, y_sample

train_dataset = CasiaDataset(X_train, y_train)
val_dataset = CasiaDataset(X_test, y_test)

ID_new = np.array([  4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,
        15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,  24.,  25.,  26.,
        27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.])#,  37.,
        # 38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,
        # 49.,  51.,  52.,  53.,  54.,  55.,  56.,  57.,  58.,  59.,  60.,
        # 61.,  62.,  63.,  64.,  65.,  66.,  67.,  68.,  69.,  70.,  71.,
        # 72.,  73.,  74.,  75.,  76.,  77.,  78.,  79.,  80.,  82.,  83.,
        # 84.,  85.,  87.,  88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,
        # 96.,  97.,  98.,  99., 100., 101., 102., 103.])

"""# Define Model"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.autograd as autograd

# Enable anomaly detection
autograd.set_detect_anomaly(True)

import copy
def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class BasicConv1d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):
        super(BasicConv1d, self).__init__()
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, bias=False, **kwargs)

    def forward(self, x):
        ret = self.conv(x)
        # print(f'Conv1d:{ret.shape}')
        return ret

class FocalConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, halving, **kwargs):
        super(FocalConv2d, self).__init__()
        self.halving = halving
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, bias=False, **kwargs).to(torch.float64)

    def forward(self, x):
        h = x.size(2)
        split_size = int(h // 2**self.halving)
        z = x.split(split_size, 2)
        z = torch.cat([self.conv(_) for _ in z], 2)
        z = F.leaky_relu(z, inplace=True)  # Apply the leaky_relu inplace

        # print(f'FocalConv2d:{z.shape}')  # Print the shape after the inplace operation

        return z

class TemporalFeatureAggregator(nn.Module):
    def __init__(self, in_channels, squeeze=4, part_num=16):
        super(TemporalFeatureAggregator, self).__init__()
        hidden_dim = int(in_channels // squeeze)
        self.part_num = part_num

        # MTB1
        conv3x1 = nn.Sequential(
                BasicConv1d(in_channels, hidden_dim, 3, padding=1),
                nn.LeakyReLU(inplace=True),
                BasicConv1d(hidden_dim, in_channels, 1))
        self.conv1d3x1 = clones(conv3x1, part_num)
        self.avg_pool3x1 = nn.AvgPool1d(3, stride=1, padding=1)
        self.max_pool3x1 = nn.MaxPool1d(3, stride=1, padding=1)

        # MTB1
        conv3x3 = nn.Sequential(
                BasicConv1d(in_channels, hidden_dim, 3, padding=1),
                nn.LeakyReLU(inplace=True),
                BasicConv1d(hidden_dim, in_channels, 3, padding=1))
        self.conv1d3x3 = clones(conv3x3, part_num)
        self.avg_pool3x3 = nn.AvgPool1d(5, stride=1, padding=2)
        self.max_pool3x3 = nn.MaxPool1d(5, stride=1, padding=2)

    def forward(self, x):
        """
          Input: x, [p, n, c, s]
        """
        # print(f'TFA input size:{x.shape}')

        p, n, c, s = x.size()
        feature = x.split(1, 0)
        x = x.view(-1, c, s)

        # MTB1: ConvNet1d & Sigmoid
        logits3x1 = torch.cat([conv(_.squeeze(0)).unsqueeze(0)
            for conv, _ in zip(self.conv1d3x1, feature)], 0)
        scores3x1 = torch.sigmoid(logits3x1)
        # MTB1: Template Function
        feature3x1 = self.avg_pool3x1(x) + self.max_pool3x1(x)
        feature3x1 = feature3x1.view(p, n, c, s)
        feature3x1 = feature3x1 * scores3x1

        # MTB2: ConvNet1d & Sigmoid
        logits3x3 = torch.cat([conv(_.squeeze(0)).unsqueeze(0)
            for conv, _ in zip(self.conv1d3x3, feature)], 0)
        scores3x3 = torch.sigmoid(logits3x3)
        # MTB2: Template Function
        feature3x3 = self.avg_pool3x3(x) + self.max_pool3x3(x)
        feature3x3 = feature3x3.view(p, n, c, s)
        feature3x3 = feature3x3 * scores3x3

        # Temporal Pooling
        ret = (feature3x1 + feature3x3).max(-1)[0]
        # print(f'Temporal Feature Aggregator:{ret.shape}')
        return ret

class GaitPart(nn.Module):
    def __init__(self, in_channels, num_classes, squeeze=4, part_num=16, halving=1):
        super(GaitPart, self).__init__()

        self.focal_conv = FocalConv2d(in_channels, 64, kernel_size=(3, 3), halving=halving)
        self.temporal_aggregator = TemporalFeatureAggregator(32, squeeze=squeeze, part_num=part_num)

        # Fully connected layer
        self.fc_final = nn.Linear(2048, num_classes)  # Adjust num_classes

    def forward(self, x):
        x = x.to(torch.float64)
        x = self.focal_conv(x)
        x = self.temporal_aggregator(x)

        # Flatten the tensor for the fully connected layer
        x = x.view(x.size(0), -1)
        x = self.fc_final(x)
        return x

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# Instantiate the model
model = GaitPart(in_channels=100, num_classes=2).to(torch.float64)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust lr as needed
batch_size = 8
# Create data loaders for training and validation
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Training loop
num_epochs = 10  # Adjust as needed
for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        inputs = inputs.to(device).to(torch.float64)
        labels = labels.to(device)

        # Reshape labels
        # labels = labels.squeeze(1)  # Remove the singleton dimension
        outputs = model(inputs)
        # print(outputs, labels)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    # Validation loop
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    # Inside your validation loop
    with torch.no_grad():
        for inputs, labels in val_loader:
            outputs = model(inputs)
            val_loss += criterion(outputs, labels).item()

            # Convert one-hot encoded labels to class indices
            _, predicted = outputs.max(1)
            predicted_indices = predicted  # predicted is already class indices
            true_indices = labels.argmax(1)  # Convert one-hot labels to class indices

            total += labels.size(0)
            correct += predicted_indices.eq(true_indices).sum().item()

    print(f"Epoch [{epoch+1}/{num_epochs}] - "
          f"Validation Loss: {val_loss/len(val_loader):.4f}, "
          f"Validation Accuracy: {(correct/total)*100:.2f}%")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

def run(train_dataset, val_dataset, in_channels, num_classes, num_epochs=10, batch_size=8, learning_rate=0.001):
    # Instantiate the model
    model = GaitPart(in_channels=in_channels, num_classes=num_classes).to(torch.float64)

    # Define loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    val_accuracies = []  # To store validation accuracies
    val_losses = []      # To store validation losses

    for epoch in range(num_epochs):
        model.train()
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            inputs = inputs.to(device).to(torch.float64)
            labels = labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                outputs = model(inputs)
                val_loss += criterion(outputs, labels).item()

                _, predicted = outputs.max(1)
                predicted_indices = predicted
                true_indices = labels.argmax(1)

                total += labels.size(0)
                correct += predicted_indices.eq(true_indices).sum().item()

        val_accuracy = (correct / total) * 100
        val_accuracies.append(val_accuracy)
        val_losses.append(val_loss / len(val_loader))

        print(f"Epoch [{epoch+1}/{num_epochs}] - "
              f"Validation Loss: {val_losses[-1]:.4f}, "
              f"Validation Accuracy: {val_accuracies[-1]:.2f}%")

    return val_accuracies, val_losses

X_train, X_test, y_train, y_test = X_train_array[0], X_test_array[0], y_train_array[0], y_test_array[0]
y_train = np.array(y_train[:],dtype='uint8')
y_train = np.eye(2)[y_train].reshape((y_train.shape[0],-1))

y_test = np.array(y_test[:],dtype='uint8')
y_test = np.eye(2)[y_test].reshape((y_test.shape[0],-1))

# Transpose the array to swap the 2nd and 4th dimensions
print(X_train.shape, X_test.shape)
X_train = np.transpose(X_train, axes=[0, 3, 1, 2])  # shape: (48, 100, 36, 20)
X_test  = np.transpose(X_test , axes=[0, 3, 1, 2])  # shape: (12, 100, 36, 20)

import pandas as pd

acc_list = np.empty((len(ID_new),1))
bal_acc_list = np.empty((len(ID_new),1))
sen_list = np.empty((len(ID_new),1))
spec_list = np.empty((len(ID_new),1))
side = 'l'
dataset_type = 'grf'
feature_type = 'image' # can be 'ws'(wavelet scattering), 'image', 'glcm', 'lbp', 'dct', 'wtA' 'wtV' 'wtH' 'wtD'(wavelet transform)

dr_array = ['none'] #  'umap','pca','dct','som','tsne', 'none', 'pca' , 'lda' , 'tsne', 'som', 'umap',
classifier = 'svm' # 'knn', 'svm'
user_idx = -1

for dr_method in dr_array:
    # for user in ID_backup:
    for user in ID_new:
        user_idx = user_idx + 1
        X_train = X_train_array[user_idx]
        X_test = X_test_array[user_idx]
        y_train = y_train_array[user_idx]
        y_test = y_test_array[user_idx]

        y_train = np.array(y_train[:],dtype='uint8')
        y_train = np.eye(2)[y_train].reshape((y_train.shape[0],-1))

        y_test = np.array(y_test[:],dtype='uint8')
        y_test = np.eye(2)[y_test].reshape((y_test.shape[0],-1))

        # Transpose the array to swap the 2nd and 4th dimensions
        print(X_train.shape, X_test.shape)
        X_train = np.transpose(X_train, axes=[0, 3, 1, 2])  # shape: (48, 100, 36, 20)
        X_test  = np.transpose(X_test , axes=[0, 3, 1, 2])  # shape: (12, 100, 36, 20)

        train_dataset = CasiaDataset(X_train, y_train)
        val_dataset = CasiaDataset(X_test, y_test)

        # Call the run function with your datasets and parameters
        val_accuracies, val_losses = run(train_dataset, val_dataset, in_channels=100, num_classes=2, num_epochs=10, batch_size=8, learning_rate=0.001)
        print(f"Validation Accuracies for user {user_idx}:", val_accuracies)
        print(f"Validation Losses:{user_idx}", val_losses)
        bal_acc_list[user_idx], acc_list[user_idx] = val_accuracies[-1], val_losses[-1]

        ##########
        # image_dataset = np.empty((labels_matrix.shape[0], 4 * grf.shape[0])) # , grf.shape[1]))

        # Call the function to generate the image dataset
        # print(X_train.shape)
        # X_train = generate_feature_dataset(y_train, X_train, dataset_type, side)

        # Print the shape of the generated image dataset
        # print(X_train.shape)

        # X_train = feature_extraction(X_train, feature_type=feature_type)

        # Call the function to generate the image dataset
        # X_test = generate_feature_dataset(y_test, X_test, dataset_type, side)

        # Print the shape of the generated image dataset
        # print(X_test.shape)

        # X_test = feature_extraction(X_test, feature_type=feature_type)
        ############
        # X_train, X_test = normalization(X_train, X_test)

        # X_train_reduced, X_test_reduced = dim_reduction(X_train, y_train, X_test, dimensionality_reduction=dr_method, n_components=3)

        # plt.figure()
        # plt.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], c=y_train)
        # plt.title(f'plot {dr_method} output distribution')
        # plt.show()

        # if classifier == 'svm':
        #     pred = classifier_svm(X_train_reduced, y_train, X_test_reduced)
        # elif classifier == 'knn':
        #     pred, accuracies, kVals = classifier_knn(KNN, X_train_reduced, y_train, X_test_reduced, y_test)

        # bal_acc_list[user_idx], acc_list[user_idx], spec_list[user_idx], sen_list[user_idx] = evaluation_metrics(y_test, pred)

    user = 0
    ones_array = np.ones((len(ID_new),1))
    data_array = np.concatenate((ones_array, ID_new.reshape(len(ID_new),1), bal_acc_list.reshape(len(ID_new),1), acc_list.reshape(len(ID_new),1), sen_list.reshape(len(ID_new),1), spec_list.reshape(len(ID_new),1)), axis=1)
    df = pd.DataFrame(data_array, columns = ['Test', 'ID', 'BalAcc', 'Loss', 'Specifity', 'Sensitivity'])
    # visualize_boxplot(df, evaluation_mertic='BalAcc', method=dataset_type)
    mean = df.mean()
    print(f'Mean for {dr_method}:\n{mean}')
    std = df.std()
    print(f'\nSTD for {dr_method}:\n{std}')
    # filepath = f'Mar15_{dataset_name}_{dataset_type}_{feature_type}_{dr_method}.xlsx'
    # df.to_excel(filepath, index=False)